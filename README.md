# ğŸ¦ BFSI Call Center AI Assistant

A lightweight, compliant, and modular AI assistant designed to handle Banking, Financial Services, and Insurance (BFSI) call center queries using a dataset-first architecture, local Small Language Model (SLM), and Retrieval-Augmented Generation (RAG).

## ğŸ“Œ Objective
The goal of this project is to build a safe, efficient, and compliance-aware AI system that:
* Runs locally using a small language model.
* Prioritizes curated dataset responses.
* Uses retrieval for complex financial queries.
* Enforces strict BFSI safety and compliance guardrails.

*This system is designed to minimize hallucination and maximize response reliability.*

## ğŸ— Architecture Overview

```text
User Query
    â†“
Guardrails Layer
    â†“
Tier 1: Dataset Similarity Match (Alpaca)
    â†“ (if strong match) â” Return Stored Response
    â†“ (if no strong match)
Tier 3: RAG (Policy Retrieval)
    â†“ (if policy-related query) â” Grounded Response Generation
    â†“ (otherwise)
Tier 2: Local Small Language Model (Fallback)
    â†“
Final Response

ğŸ§  Core Components
1ï¸âƒ£ Guardrails Layer (Compliance First)
Ensures BFSI regulatory compliance by enforcing:

No exposure of sensitive data.

No identity-based personalization.

No guessing of financial numbers.

No generation of fake policies.

Rejection of out-of-domain queries.

2ï¸âƒ£ Dataset Layer (Primary Response Engine)
Dataset: 151+ Alpaca-formatted BFSI samples.

Quality: Professional and standardized responses.

Search: Semantic similarity search using sentence-transformers.

Behavior: Deterministic.

Note: If a strong similarity match is found, the stored response is returned directly. This minimizes hallucination and ensures compliance.

3ï¸âƒ£ Small Local Language Model (SLM)
Model: TinyLlama-1.1B-Chat

Execution: Runs locally on CPU.

Usage: Used only when a dataset match fails.

Safety: Prompt-conditioned for compliance safety.

Note: Due to hardware and time constraints, prompt conditioning was used instead of full LoRA fine-tuning. The architecture supports future fine-tuning if required.

4ï¸âƒ£ RAG Layer (Knowledge Retrieval)
Storage: Structured policy documents stored in data/knowledge_docs.

Mechanism: Semantic retrieval using embeddings for context-grounded generation.

Used For: Interest explanations, EMI breakdowns, penalty rules, and loan approval policies.

Benefit: Ensures grounded responses for complex financial queries.

ğŸ“‚ Project Structure
bfsi-call-center-ai-assistant/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ alpaca_dataset.json
â”‚   â””â”€â”€ knowledge_docs/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ similarity.py
â”‚   â”œâ”€â”€ guardrails.py
â”‚   â”œâ”€â”€ slm.py
â”‚   â”œâ”€â”€ rag.py
â”‚   â””â”€â”€ pipeline.py
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸš€ How to Run Locally1ï¸âƒ£ Clone RepositoryBashgit clone [https://github.com/PandiKabilesh2006/bfsi-call-center-ai-assistant.git](https://github.com/PandiKabilesh2006/bfsi-call-center-ai-assistant.git)
cd bfsi-call-center-ai-assistant
2ï¸âƒ£ Create Virtual EnvironmentBashpython -m venv venv
venv\Scripts\activate   # Windows
# source venv/bin/activate # Mac/Linux
3ï¸âƒ£ Install DependenciesBashpip install -r requirements.txt
4ï¸âƒ£ Run Streamlit AppBashstreamlit run app.py
ğŸ§ª Example Query RoutingQueryResponse SourceHow is EMI calculated?DatasetExplain loan approval criteriaRAGExplain amortizationSLMWhat is my account number?GuardrailsğŸ›¡ Safety & Compliance DesignDesigned to reflect real-world BFSI AI deployment standards, this system enforces:Deterministic dataset-first responses.Strict guardrail filtering.No financial number guessing.No exposure of customer identity data.Grounded RAG responses for policy queries.ğŸ“Š Scalability & MaintainabilityModular architectureEasily extensible datasetReplaceable SLM modelUpdatable knowledge baseClear separation of concernsFuture Improvements:LoRA fine-tuning on the Alpaca dataset.Advanced semantic routing.Conversation memory support.Logging & monitoring integration.ğŸ¯ Key Design DecisionsWhy Dataset First? To minimize hallucination and ensure standardized, compliance-safe responses.Why RAG Before Generic SLM? To ground policy-related answers and reduce the risk of misinformation.Why Local SLM? To ensure data privacy and offline capability.ğŸ“¦ Deliverables Covered[x] 150+ Alpaca formatted dataset[x] Local SLM integration[x] Structured RAG knowledge base[x] Working end-to-end Streamlit demo[x] Modular architecture with documentationğŸ§‘â€ğŸ’» Author Pandi Kabilesh AI/ML Enthusiast | Aspiring Machine Learning Engineer